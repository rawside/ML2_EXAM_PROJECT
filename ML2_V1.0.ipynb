{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdTKbUAwNMMXvxLJP9eJUN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning II - Exam-Project - Sascha Pfeiffer "
      ],
      "metadata": {
        "id": "JvUSOnLg40Ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "---\n",
        "\n",
        "Welcome to my Python Notebook, where I'm undertaking a study of sentiment analysis in the realm of cryptocurrency, with a particular emphasis on the social media platform Twitter.\n",
        "\n",
        "Cryptocurrencies are known for their price volatility, which can be influenced by a wide array of factors. Among these, I'm examining the role of Twitter, a platform where discussions about cryptocurrencies are constantly happening in real time.\n",
        "\n",
        "In this project, I'll be gathering tweets related to different cryptocurrencies using the Twitter API, and then applying sentiment analysis to this data. The aim here is to determine the underlying sentiments expressed in tweets about specific cryptocurrencies.\n",
        "\n",
        "I plan to train my own model and also leverage the OpenAI API to ascertain the sentiment of these tweets. Over a certain period, I'll amass data from Twitter and use both my model and the OpenAI API to predict the sentiment expressed in these tweets.\n",
        "\n",
        "Furthermore, I'll create a function to track how sentiment changes over time. For this analysis, I'll only consider tweets where the sentiment predictions from my model and OpenAI coincide. This accumulated data could potentially be useful for future model training.\n",
        "\n",
        "So, I invite you to join me on this scholarly journey that merges Python, cryptocurrencies, Twitter, and sentiment analysis. Let's explore what Twitter discussions reveal about sentiments within the crypto market!"
      ],
      "metadata": {
        "id": "tAl_DFFnFvXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the credentials\n",
        "---\n",
        "\n",
        "A fellow student has to present this project, therefore I implement a function to upload the credentials in a JSON-Format and save them in variables for later use. She or he can request the JSON-File by e-mail from pfeifsas@\n",
        "\n",
        "This process ensures the secure and organized handling of project credentials."
      ],
      "metadata": {
        "id": "a8wLA7fAHMv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import json\n",
        "\n",
        "# Use files.upload to produce the \"Choose Files\" button below, then select your file.\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Use io.BytesIO to decode the file, then json.load to open it.\n",
        "file = io.BytesIO(uploaded['credentials.json'])\n",
        "credentials = json.load(file)\n",
        "\n",
        "# Use Python list comprehension to save each credential to a separate variable.\n",
        "TWITTER_CONSUMER_KEY = credentials['TWITTER_CONSUMER_KEY']\n",
        "TWITTER_CONSUMER_SECRET = credentials['TWITTER_CONSUMER_SECRET']\n",
        "TWITTER_ACCESS_TOKEN = credentials['TWITTER_ACCESS_TOKEN']\n",
        "TWITTER_ACCESS_TOKEN_SECRET = credentials['TWITTER_ACCESS_TOKEN_SECRET']\n",
        "BEARER_TOKEN = credentials['BEARER_TOKEN']\n",
        "GPT_SECRET_KEY = credentials['GPT_SECRET_KEY']\n",
        "MONGO_CONNECTION_STRING = credentials['MONGO_CONNECTION_STRING']\n"
      ],
      "metadata": {
        "id": "Hv5-l7a_IX5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and import all the needed libraries and dependencies\n",
        "---\n",
        "\n",
        "This code installs all the necessary libraries and dependencies."
      ],
      "metadata": {
        "id": "EgecDSjgUhth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the Libraries"
      ],
      "metadata": {
        "id": "cDFMsgKHkZsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install openai\n",
        "!pip install 'pymongo[srv]'"
      ],
      "metadata": {
        "id": "CNdS_gdRkbfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the dependencies"
      ],
      "metadata": {
        "id": "gvMKXXsekUP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# plotting\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report,roc_auc_score, roc_curve\n",
        "\n",
        "# utilities\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.nn import functional as F\n",
        "import openai\n",
        "from pymongo.mongo_client import MongoClient\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n"
      ],
      "metadata": {
        "id": "ZBas_IGja3C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing and converting the dataset\n",
        "---\n",
        "In order to train my sentiment analysis model, I needed a dataset that contained labeled tweets. Since tweets have a unique structure due to their character limit, I discovered the [sentiment140](https://huggingface.co/datasets/sentiment140) dataset on Huggingface, which consists of 1.6 million sentiment-labeled tweets.\n",
        "\n",
        "Huggingface offers the option to download the dataset as a CSV file, or it can be imported directly using the datasets library.\n",
        "\n",
        "Rather than downloading and importing a CSV file, I opted to utilize the datasets library from Huggingface to load the initial training dataset. Once loaded, I converted it into a pandas dataframe for further processing."
      ],
      "metadata": {
        "id": "oPXLEVNYicv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"sentiment140\")\n",
        "\n",
        "# Access a split and convert to a pandas dataframe\n",
        "df = dataset['train'].to_pandas()\n",
        "\n",
        "# Removing the unnecessary columns.\n",
        "df = df[['sentiment','text']]\n",
        "\n",
        "# Replacing the values to ease understanding.\n",
        "df['sentiment'] = df['sentiment'].replace(4,1)\n",
        "\n",
        "# Plotting the distribution for dataset.\n",
        "ax = df.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n",
        "                                               legend=False)\n",
        "ax.set_xticklabels(['Negative','Positive'], rotation=0)\n",
        "\n",
        "# Storing data in lists.\n",
        "text, sentiment = list(df['text']), list(df['sentiment'])"
      ],
      "metadata": {
        "id": "qccGM9saa5aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "MKbCRgYKsDQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a preprocessing function\n",
        "--- \n",
        "\n",
        "In my initial implementation, I manually defined a stopword list. However, I decided to enhance the quality of the stopwords by integrating the nltk library, which provides a state-of-the-art stopword list. This improved stopword list was then incorporated into my preprocessing function. The original code is commented out below:"
      ],
      "metadata": {
        "id": "q6xiaqqQd7pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''## Defining set containing all stopwords in english. \n",
        "\n",
        "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
        "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']'''"
      ],
      "metadata": {
        "id": "V9BdstExdBqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The function provided below is used to preprocess tweets for sentiment analysis, both for training and prediction purposes."
      ],
      "metadata": {
        "id": "hYwbGydwR_rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##First attempt:\n",
        "\n",
        "There is an updated version of the preprocessing function below."
      ],
      "metadata": {
        "id": "_t0NU1VeAHqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''def preprocess(textdata):\n",
        "    processedText = []\n",
        "\n",
        "    # Create Lemmatizer and Stemmer.\n",
        "    wordLemm = WordNetLemmatizer()\n",
        "\n",
        "    # Defining regex patterns.\n",
        "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "    userPattern       = '@[^\\s]+'\n",
        "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
        "    sequencePattern   = r\"(.)\\1\\1+\"\n",
        "    seqReplacePattern = r\"\\1\\1\"\n",
        "    emoticonPattern   = r\"[:;=8][\\-o\\*\\']?[\\)\\]\\(\\[dDpP/\\\\OpP3]\"\n",
        "\n",
        "    for tweet in textdata:\n",
        "        if tweet is not None:   # skip None values\n",
        "            tweet = tweet.lower()\n",
        "\n",
        "            # Replace all URls with 'URL'\n",
        "            tweet = re.sub(urlPattern,' URL',tweet)       \n",
        "            # Replace @USERNAME to 'USER'.\n",
        "            tweet = re.sub(userPattern,' USER', tweet)        \n",
        "            # Replace all non alphabets.\n",
        "            tweet = re.sub(alphaPattern, \" \", tweet)\n",
        "            # Replace 3 or more consecutive letters by 2 letter.\n",
        "            tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
        "            # Replace emoticons with an empty string\n",
        "            tweet = re.sub(emoticonPattern, '', tweet)\n",
        "\n",
        "            tweetwords = ''\n",
        "            for word in tweet.split():\n",
        "                # Checking if the word is a stopword.\n",
        "                if word not in stopwordlist:\n",
        "                    if len(word)>1:\n",
        "                        # Lemmatizing the word.\n",
        "                        word = wordLemm.lemmatize(word)\n",
        "                        tweetwords += (word+' ')\n",
        "                \n",
        "            processedText.append(tweetwords)\n",
        "        \n",
        "    return processedText'''\n"
      ],
      "metadata": {
        "id": "7o93kjOjeK9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Second attempt:\n",
        "\n",
        "###Optimizations:\n",
        "- Utilizing the NLTK stopword library to improve preprocessing efficiency.\n",
        "- Incorporating a function that reduces consecutive occurrences of the word \"USER\" to a single mention when it follows another instance of \"USER\" within a tweet, thereby improving the accuracy of the sentiment analysis.\n",
        "\n",
        "Please refer to the subsequent sections of the same notebook for a detailed explanation of why I made multiple changes to the \"USER\" mentions in the tweets.\n"
      ],
      "metadata": {
        "id": "TUcRPXzJAJx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwordlist = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(textdata):\n",
        "    processedText = []\n",
        "\n",
        "    # Create Lemmatizer\n",
        "    wordLemm = WordNetLemmatizer()\n",
        "\n",
        "    # Defining regex patterns.\n",
        "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "    userPattern       = '@[^\\s]+'\n",
        "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
        "    sequencePattern   = r\"(.)\\1\\1+\"\n",
        "    seqReplacePattern = r\"\\1\\1\"\n",
        "    emoticonPattern   = r\"[:;=8][\\-o\\*\\']?[\\)\\]\\(\\[dDpP/\\\\OpP3]\"\n",
        "\n",
        "    for tweet in textdata:\n",
        "        if tweet is not None:   # Skip None values\n",
        "            tweet = tweet.lower()\n",
        "\n",
        "            # Replace all URLs with 'URL'\n",
        "            tweet = re.sub(urlPattern,' URL',tweet)       \n",
        "            # Replace @USERNAME with 'USER'.\n",
        "            tweet = re.sub(userPattern,' USER', tweet)        \n",
        "            # Replace all non-alphabets.\n",
        "            tweet = re.sub(alphaPattern, \" \", tweet)\n",
        "            # Replace 3 or more consecutive letters by 2 letters.\n",
        "            tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
        "            # Replace emoticons with an empty string.\n",
        "            tweet = re.sub(emoticonPattern, '', tweet)\n",
        "\n",
        "            tweetwords = ''\n",
        "            for word in tweet.split():\n",
        "                # Checking if the word is a stopword.\n",
        "                if word not in stopwordlist:\n",
        "                    if len(word) > 1:\n",
        "                        # Lemmatize the word.\n",
        "                        word = wordLemm.lemmatize(word)\n",
        "                        tweetwords += (word+' ')\n",
        "                \n",
        "            tweetwords = reduce_user_mentions([tweetwords])  # Reduce multiple user mentions\n",
        "\n",
        "            processedText.append(tweetwords[0])\n",
        "\n",
        "    return processedText\n",
        "\n",
        "\n",
        "def reduce_user_mentions(tweets):\n",
        "    processed_tweets = []\n",
        "    for tweet in tweets:\n",
        "        words = tweet.split()\n",
        "        processed_words = [words[i] if words[i] != 'USER' or (i > 0 and words[i-1] != 'USER') else '' for i in range(len(words))]\n",
        "        processed_tweet = ' '.join(processed_words).strip()\n",
        "        processed_tweets.append(processed_tweet)\n",
        "    return processed_tweets\n"
      ],
      "metadata": {
        "id": "OaPCMpBz_vIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing tweets in  dataset\n",
        "---\n",
        "\n",
        "In this step, I am performing preprocessing on all the tweets in my training set. The purpose of this preprocessing step is to clean and transform the tweet data before further analysis or model training."
      ],
      "metadata": {
        "id": "neKMNEeRArjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = time.time()\n",
        "processedtext = preprocess(text)\n",
        "print(f'Text Preprocessing complete.')\n",
        "print(f'Time Taken: {round(time.time()-t)} seconds')"
      ],
      "metadata": {
        "id": "qi0C_3yxSkNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating word clouds\n",
        "---\n",
        "After preprocessing the tweets, I am using a word cloud to visually represent the most frequent words in both positive and negative tweets. By creating word clouds for each sentiment category, I can gain insights into the key themes and sentiments expressed in the dataset. This visual representation allows me to observe the prominent words associated with positive and negative sentiments, helping me understand the overall sentiment distribution and potentially identify important patterns or trends in the data."
      ],
      "metadata": {
        "id": "jGnJUJMRUmmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word-Cloud for positive tweets"
      ],
      "metadata": {
        "id": "WJxBsaySU1Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_pos = processedtext[800000:]\n",
        "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
        "              collocations=False).generate(\" \".join(data_pos))\n",
        "plt.figure(figsize = (20,20))\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "PUEFPQaWU6EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word-Cloud for negative tweets"
      ],
      "metadata": {
        "id": "b_2xv9NMCDw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_neg = processedtext[:800000]\n",
        "plt.figure(figsize = (20,20))\n",
        "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
        "               collocations=False).generate(\" \".join(data_neg))\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "d0q170rTUpWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utilizing N-grams to enhance contextual analysis\n",
        "---\n",
        "\n",
        "Now I extract N-grams from preprocessed positive and negative tweet datasets. The code utilizes the extract_ngrams function to extract N-grams from a list of preprocessed tweets. The value of n is defined as 3, indicating trigrams (sequences of three words).\n",
        "\n",
        "Trigrams are then extracted from the positive and negative tweet datasets using the extract_ngrams function. The frequency of these trigrams is calculated using the Counter class from the collections module.\n",
        "\n",
        "The output displays the most common trigrams in positive and negative tweets, showcasing the language patterns associated with each sentiment category."
      ],
      "metadata": {
        "id": "IXjhtAvjD5B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract N-grams from a list of preprocessed tweets\n",
        "def extract_ngrams(tweets, n):\n",
        "    ngrams = []\n",
        "    for tweet in tweets:\n",
        "        words = tweet.split()\n",
        "        ngrams.extend([' '.join(words[i:i+n]) for i in range(len(words)-n+1)])\n",
        "    return ngrams\n",
        "\n",
        "# Define the value of N for N-grams\n",
        "n = 3\n",
        "\n",
        "# Extract N-grams from positive and negative tweet datasets\n",
        "positive_ngrams = extract_ngrams(data_pos, n)\n",
        "negative_ngrams = extract_ngrams(data_neg, n)\n",
        "\n",
        "# Calculate the frequency of N-grams\n",
        "positive_ngram_freq = Counter(positive_ngrams)\n",
        "negative_ngram_freq = Counter(negative_ngrams)\n",
        "\n",
        "# Print the most common N-grams for positive and negative tweets\n",
        "print(\"Most common N-grams in positive tweets:\")\n",
        "print(positive_ngram_freq.most_common(20))\n",
        "\n",
        "print(\"\\nMost common N-grams in negative tweets:\")\n",
        "print(negative_ngram_freq.most_common(20))\n"
      ],
      "metadata": {
        "id": "I_6DVUgqDb3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##First attempt:\n",
        "\n",
        "The analysis of the most common N-grams in positive and negative tweets reveals distinct language patterns associated with each sentiment category. Positive N-grams include gratitude, positive expressions, laughter, and greetings, while negative N-grams consist of disappointment, regrets, empathy, and negative perceptions. This insight enhances our understanding of sentiment trends in the dataset and can contribute to improving sentiment analysis models.\n",
        "\n",
        "The analysis of N-grams reveals the prevalence of the following word combinations in the dataset:\n",
        "- For positive tweets: The most frequent N-gram is 'USER USER USER' with 8509 mentions.\n",
        "- For negative tweets: The most frequent N-gram is also 'USER USER USER' with 2210 mentions.\n",
        "\n",
        "I enhanced the preprocessing function by incorporating a new feature that reduces consecutive occurrences of the word \"USER\" to a single mention when it follows another instance of \"USER\" within a tweet, resulting in improved preprocessing and more accurate sentiment analysis. \n",
        "\n",
        "The reason for this enhancement is to improve the quality and effectiveness of the sentiment analysis process. In many cases, tweets may contain multiple consecutive mentions of \"USER\" without providing any additional valuable information for sentiment analysis. By reducing multiple mentions of \"USER\" to a single mention, we simplify the tweet representation and eliminate any redundancy caused by repetitive user mentions.\n",
        "\n",
        "This enhancement helps to streamline the sentiment analysis process by focusing on the essential content of the tweet while disregarding repeated user mentions. It ensures that the sentiment analysis algorithm can better capture the sentiment-related words and phrases in the tweet without being influenced by excessive repetitions of the \"USER\" word."
      ],
      "metadata": {
        "id": "ko0oiR8ODavc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#New results after optimization of the preprocessing function\n",
        "\n",
        "Instead of the previous occurrence of \"USER USER USER,\" the most common 3-word combinations have been updated:\n",
        "\n",
        "- In positive tweets, the most frequent N-gram is 'happy mother day' with 2033 mentions.\n",
        "- In negative tweets, the most frequent N-gram is 'wish could go' with 921 mentions.\n",
        "\n",
        "These updated N-grams provide more meaningful insights in the context of sentiment analysis, as they reflect specific phrases related to positive and negative sentiments.\n"
      ],
      "metadata": {
        "id": "MYkrOGITO1hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test-Split"
      ],
      "metadata": {
        "id": "DxANHPZjgZeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n",
        "                                                    test_size = 0.05, random_state = 0)\n",
        "print(f'Data Split done.')\n",
        "\n"
      ],
      "metadata": {
        "id": "Wn6oqImYgYo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF-Vectorizer\n",
        "\n",
        "I'm using the TfidfVectorizer to convert the text data into numerical features. By setting `ngram_range=(1,2)`, I consider both single words and word pairs. After fitting the vectorizer to the training data, I print the confirmation message and display the number of feature words extracted."
      ],
      "metadata": {
        "id": "kv0RbI7Bgkbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the vectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
        "\n",
        "# Fit it to the training data\n",
        "vectorizer.fit(X_train)\n",
        "\n",
        "print(f'Vectoriser fitted.')\n",
        "print('No. of feature_words: ', len(vectorizer.get_feature_names_out()))"
      ],
      "metadata": {
        "id": "TkLQVID8gjyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My goal is to convert messy, unstructured text data into numerical vectors that machine learning models can process. To do this, I use the `TfidfVectorizer` from the `sklearn` library.\n",
        "\n",
        "These vectorizer has key parameters such as `max_features` and `ngram_range`. `Max_features` limits the number of unique words considered. Varying this number can help manage noise and complexity in my models, but it's a balancing act. `Ngram_range`, on the other hand, determines the length of word groups used. A setting like (1,3) can help capture more context by considering unigrams, bigrams, and trigrams. \n",
        "\n",
        "But here's the thing: when I experimented with these parameters, my models' performance didn't improve. I tried both increasing and decreasing `max_features`, and experimented with larger n-grams. It seems the initial settings were already well-tuned for my task and data, and adding complexity didn't help but possibly added more noise or overfitting. So, I decided to stick with my initial vectorization settings, reminding me that machine learning often involves a well-guided trial and error process."
      ],
      "metadata": {
        "id": "BPHw8br_9b1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Initialize the vectorizer with desired parameters\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=600000)\n",
        "\n",
        "# Fit it to the training data\n",
        "vectorizer.fit(X_train)\n",
        "\n",
        "print(f'Vectorizer fitted.')\n",
        "print('No. of feature_words: ', len(vectorizer.get_feature_names_out()))'''"
      ],
      "metadata": {
        "id": "7rqEFCTgx59r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming the data set"
      ],
      "metadata": {
        "id": "OxSTL_3po5Nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = vectorizer.transform(X_train)\n",
        "X_test  = vectorizer.transform(X_test)\n",
        "print(f'Data Transformed.')"
      ],
      "metadata": {
        "id": "hZTnxG5lo45g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#function to evaluate different models\n",
        "---\n",
        "I first implemented a basic function that plotted a confusion-matrix for each model to be trained."
      ],
      "metadata": {
        "id": "PN4c_WnjjH99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''def model_Evaluate(model):\n",
        "    \n",
        "    # Predict values for Test dataset\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Print the evaluation metrics for the dataset.\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    \n",
        "    # Compute and plot the Confusion matrix\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    categories  = ['Negative','Positive']\n",
        "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
        "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
        "                xticklabels = categories, yticklabels = categories)\n",
        "\n",
        "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)'''"
      ],
      "metadata": {
        "id": "iMBWvrmkjLVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The improved function `model_Evaluate()` helps me assess the performance of a binary classification model. It predicts values for my test set and generates a classification report, including metrics like precision, recall.\n",
        "\n",
        "It then generates a confusion matrix, visualized as a heatmap. This lets me see the true positives, false positives, true negatives, and false negatives at a glance. I also calculate sensitivity (or recall), specificity, and overall accuracy, each giving me different insights into my model's performance.\n",
        "\n",
        "Finally, I plot a Receiver Operating Characteristic (ROC) curve and calculate the area under the curve (AUC). This gives me a single, summarizing figure for the performance of my model, showing how well it distinguishes between classes.\n",
        "\n",
        "Comparing all these metrics gives me a rounded view of my model's performance. Each one tells me something different and important about how well my model is doing and where it might be going wrong. Depending on my task, some metrics may be more important than others, and this function allows me to consider all these factors."
      ],
      "metadata": {
        "id": "2Zy3oeeNQsSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_Evaluate(model):\n",
        "    # Predict values for the test dataset\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Print the evaluation metrics for the dataset.\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Compute and plot the Confusion matrix\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    categories = ['Negative', 'Positive']\n",
        "    group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
        "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names, group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2, 2)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    sns.heatmap(cf_matrix, annot=labels, cmap='Blues', fmt='', xticklabels=categories, yticklabels=categories,\n",
        "                ax=ax1)\n",
        "    ax1.set_xlabel(\"Predicted values\", fontdict={'size': 14}, labelpad=10)\n",
        "    ax1.set_ylabel(\"Actual values\", fontdict={'size': 14}, labelpad=10)\n",
        "    ax1.set_title(\"Confusion Matrix\", fontdict={'size': 18}, pad=20)\n",
        "\n",
        "    # Compute and plot ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    # Round the roc_auc to two decimal places\n",
        "    roc_auc = round(roc_auc, 2)\n",
        "\n",
        "    # Plot ROC Curve\n",
        "    ax2.plot(fpr, tpr, label='ROC Curve (area = %0.2f)' % roc_auc)\n",
        "    ax2.plot([0, 1], [0, 1], 'k--')\n",
        "    ax2.set_xlim([0.0, 1.0])\n",
        "    ax2.set_ylim([0.0, 1.05])\n",
        "    ax2.set_xlabel('False Positive Rate')\n",
        "    ax2.set_ylabel('True Positive Rate')\n",
        "    ax2.set_title('Receiver Operating Characteristic')\n",
        "    ax2.legend(loc=\"lower right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "zWMOhG3YQr_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Selection Strategy\n",
        "---\n",
        "I am adopting a dual approach for sentiment prediction. The first part of this strategy involves training a custom model, while the second part harnesses the capabilities of the GPT-API for sentiment analysis.\n",
        "\n",
        "Considering the utilization of the GPT-API, I have decided to train a relatively simpler custom model. I'll be training and comparing the performance of three distinct models:\n",
        "\n",
        "- Bernoulli Naive Bayes Model\n",
        "- Linear Support Vector Machine Model\n",
        "- Logistic Regression Model\n",
        "\n",
        "Among these, the model that delivers the highest performance will be selected for further optimization. This optimized model will then be used to predict the sentiments of tweets, which I'll retrieve directly from the Twitter API.\n"
      ],
      "metadata": {
        "id": "wn99XVyiKxLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BernoulliNB Model\n",
        "\n",
        "This is a probabilistic classifier that makes use of Bayes' Theorem with strong independence assumptions. It is particularly suitable for data that can be binary, like the presence or absence of a word in text."
      ],
      "metadata": {
        "id": "JRourgTujVsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BNBmodel = BernoulliNB(alpha = 2)\n",
        "BNBmodel.fit(X_train, y_train)\n",
        "model_Evaluate(BNBmodel)"
      ],
      "metadata": {
        "id": "pHtZpEx0jOnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear SVM Model\n",
        "\n",
        "This is a maximum-margin classifier which works by constructing a hyperplane or a set of hyperplanes in a high or infinite dimensional space, making it a robust model for text classification tasks."
      ],
      "metadata": {
        "id": "7d1-RxRLpWwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVCmodel = LinearSVC()\n",
        "SVCmodel.fit(X_train, y_train)\n",
        "model_Evaluate(SVCmodel)"
      ],
      "metadata": {
        "id": "vJ1mBJvKpd6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Model \n",
        "\n",
        "This is a statistical model that uses a logistic function to model a binary dependent variable. In the context of sentiment analysis, it predicts the probability of a particular sentiment (positive or negative) based on input features."
      ],
      "metadata": {
        "id": "ZcNB4hOqp1_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
        "LRmodel.fit(X_train, y_train)\n",
        "model_Evaluate(LRmodel)"
      ],
      "metadata": {
        "id": "enEjHBzfp1yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comparing the performance of the different models\n",
        "\n",
        "In terms of sensitivity (recall), the Logistic Regression model performs the best with a value of 81.16%. It indicates that the model correctly identifies 81.16% of positive (1) instances.\n",
        "\n",
        "Regarding specificity, the Logistic Regression model also performs the best with a specificity of 77.94%. It indicates that the model correctly identifies 77.94% of negative (0) instances.\n",
        "\n",
        "In terms of overall accuracy, the Logistic Regression model achieves an accuracy of 79.55%, indicating the percentage of correctly predicted instances out of the total dataset.\n",
        "\n",
        "The ROC AUC score is the same for the first two models, which is 78.00%. For the Logistic Regression Model its slightly better with 80%.\n",
        "\n",
        "Based on these metrics, the Logistic Regression model appears to be the most balanced and accurate among the three models, considering both sensitivity and specificity."
      ],
      "metadata": {
        "id": "VhW6XCt4MF4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizing the chosen logistic regression model\n",
        "\n",
        "##**This operation needs a lot of time, i would not recommend to restart it!**\n",
        "\n",
        "To optimize the logistic regression model, I will utilize a grid search with K-Fold Cross-Validation (K = 5) to evaluate the model's performance using different hyperparameter combinations. The grid search will consider various metrics and assess their impact on the model's accuracy. During the evaluation process, the code will display the progress, allowing me to track the performance of each combination. After completing the grid search, the best hyperparameters and their corresponding score will be printed, providing valuable insights to improve the model's predictive capabilities and overall performance."
      ],
      "metadata": {
        "id": "h8A3M4HM_rgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the logistic regression model\n",
        "LRmodel = LogisticRegression(n_jobs=-1)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 2, 5, 10],\n",
        "    'max_iter': [100, 500, 1000],\n",
        "}\n",
        "\n",
        "# Create the grid search object\n",
        "grid_search = GridSearchCV(LRmodel, param_grid, cv=5, scoring='accuracy', verbose=2)\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and the corresponding score\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "XRywQs8Z_5-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Result from the Grid-Search\n",
        "\n",
        "The best hyperparameter combination according to the Grid-Search is the following:\n",
        "\n",
        "Best Hyperparameters: {'C': 1, 'max_iter': 100}\n",
        "Best Score: 0.793633552631579"
      ],
      "metadata": {
        "id": "EfllmvqKPPXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train the model with the new hyperparameter\n",
        "Now I train and evaluate the model with those hyperparameters"
      ],
      "metadata": {
        "id": "v2tbA_xvyyK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LRmodel = LogisticRegression(C = 1, max_iter = 100, n_jobs=-1)\n",
        "LRmodel.fit(X_train, y_train)\n",
        "model_Evaluate(LRmodel)"
      ],
      "metadata": {
        "id": "kervJkAePgde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The revised accuracy for the Logistic Regression model is 79.61%, showing a slight improvement compared to the previous value of 79.55%. The ROC AUC is still at 80%."
      ],
      "metadata": {
        "id": "ArJ37ilng88s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data from the Twitter-API\n",
        "---\n",
        "In this section, I am utilizing the Twitter API to fetch a specific number of tweets. To ensure the quality of the retrieved tweets, I specify a particular currency to be mentioned in the tweets while excluding commonly used spam-bot words. Additionally, I exclude mentions of \"NFT\" and \"NFTs\" as they are not relevant to this project focused on crypto-currencies. This approach helps filter out spam and ensure the retrieved tweets are relevant to the desired topic."
      ],
      "metadata": {
        "id": "JPToCpS5q6up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
        "\n",
        "# Hardcoded query and excluded words to avoid too many spam-tweets from bots\n",
        "query = \"Chiliz\"\n",
        "excluded_words = ['airdrop', 'bot', 'retweet', 'retweeted', 'RT', 'wallet', 'mint', 'ticket', 'drop', 'opensea', 'blur', 'NFT', 'NFTs', 'giveaway', 'announce', 'announcement']\n",
        "\n",
        "# Construct the excluded words portion of the query\n",
        "excluded_query = ' '.join(f'-{word}' for word in excluded_words)\n",
        "\n",
        "# Combine the query and excluded words\n",
        "full_query = f'{query} {excluded_query}'\n",
        "\n",
        "# Set query parameters\n",
        "query_params = {'query': full_query, 'tweet.fields': 'author_id', 'max_results': 50}\n",
        "\n",
        "def bearer_oauth(r):\n",
        "    r.headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n",
        "    r.headers[\"User-Agent\"] = \"v2FilteredStreamPython\"\n",
        "    return r\n",
        "\n",
        "def connect_to_endpoint(url, params):\n",
        "    response = requests.request(\"GET\", url, auth=bearer_oauth, params=params)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(response.status_code, response.text)\n",
        "    return response.json()\n",
        "\n",
        "json_response = connect_to_endpoint(search_url, query_params)\n",
        "\n",
        "# Convert the response to a DataFrame\n",
        "data = [{'id': tweet['id'], 'text': tweet['text'], 'query': query} for tweet in json_response['data']]\n",
        "loaded_tweets = pd.DataFrame(data)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(loaded_tweets)\n"
      ],
      "metadata": {
        "id": "7dXCrm7Z2T6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language recognition\n",
        "---\n",
        "\n",
        "In previous versions of the Twitter API, developers had the capability to select the language of the tweets directly from the API. However, this functionality is no longer available. To overcome this limitation, I am utilizing a pre-trained language detection model to filter and retain only English tweets. This ensures that the tweets used for training and prediction align with the language-specific focus of my model, which is trained exclusively on English tweets."
      ],
      "metadata": {
        "id": "Ze1hEWdz8wpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")"
      ],
      "metadata": {
        "id": "1CYittLB9Zn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function to get a predicted label for the text of a tweets\n",
        "def predict_language(text):\n",
        "    # tokenize the text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    \n",
        "    # run the text through the model and get the logits\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # compute the probabilities from the logits\n",
        "    probabilities = F.softmax(logits, dim=1).detach().numpy()\n",
        "    \n",
        "    # get the label of the highest probability\n",
        "    predicted_label = model.config.id2label[probabilities.argmax()]\n",
        "    \n",
        "    return predicted_label\n",
        "\n",
        "# create a new column with the predicted language\n",
        "loaded_tweets['language'] = loaded_tweets['text'].apply(predict_language)\n",
        "\n",
        "# filter out rows that are not in English\n",
        "english_tweets = loaded_tweets[loaded_tweets['language'] == 'en']\n"
      ],
      "metadata": {
        "id": "NCJ0Us-899Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(english_tweets)"
      ],
      "metadata": {
        "id": "1AJ6HCfhn4aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get a Sentiment from the GPT-API for the english tweets\n",
        "---\n",
        "\n",
        "To obtain the sentiment for each English tweet, I retrieve them individually from the GPT-API. The dataset is then enhanced by adding the sentiment obtained from the API for each respective tweet. The sentiment prediction is encoded as 1 for positive sentiment and 0 for negative sentiment.\n",
        "\n",
        "For the prediction task, I opt to use the \"text-ada-0001\" engine. This decision was primarily driven by financial considerations rather than quality considerations. While the \"Davinci\" model may be more powerful, its cost is approximately 50 times higher than that of the \"text-ada-0001\" model."
      ],
      "metadata": {
        "id": "dNTD9OxC4k_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **First attempt - please do not rerun this code!**\n",
        "\n",
        "I documented the evaluation process to test the quality of predictions made by the GPT model. The details of this evaluation are provided at the end of this notebook"
      ],
      "metadata": {
        "id": "ODsIgYXgePuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Set up  OpenAI API credentials\n",
        "openai.api_key = GPT_SECRET_KEY\n",
        "\n",
        "# Set up empty lists to store tweet IDs, tweet texts, and predictions\n",
        "tweet_ids = []\n",
        "tweet_texts = []\n",
        "predictions = []\n",
        "\n",
        "# Iterate over each tweet in the loaded dataset\n",
        "for index, row in english_tweets.iterrows():\n",
        "    tweet_id = row['id']\n",
        "    tweet_text = row['text']\n",
        "    \n",
        "    # Append tweet ID and text to the respective lists\n",
        "    tweet_ids.append(tweet_id)\n",
        "    tweet_texts.append(tweet_text)\n",
        "\n",
        "    # Set up your OpenAI API request\n",
        "    prompt = f\"Analyze the sentiment of the following tweet: '{tweet_text}'. Answer with 'positive' or 'negative' depending on the sentiment, just one word in the answer\"\n",
        "    \n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-ada-001\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.0\n",
        "    )\n",
        "    \n",
        "    # Extract the predicted sentiment from the OpenAI API response\n",
        "    sentiment = response.choices[0].text.strip()\n",
        "    \n",
        "    # Append the predicted sentiment to the list of predictions\n",
        "    predictions.append(sentiment)\n",
        "    \n",
        "# Create a new DataFrame with tweet IDs, texts, and predictions\n",
        "updated_data = pd.DataFrame({'id': tweet_ids, 'text': tweet_texts, 'query': english_tweets['query'], 'gpt_pred': predictions})\n",
        "\n",
        "# Replace positive/negative in 'gpt_pred' column with 1/0 and '.' with 'N/A'\n",
        "updated_data['gpt_pred'] = updated_data['gpt_pred'].apply(lambda x: 1 if 'positive' in str(x).lower() else (0 if 'negative' in str(x).lower() else 'N/A'))'''\n"
      ],
      "metadata": {
        "id": "beDHraF35KDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimized Code**\n",
        "\n",
        "In this code, I prioritized the optimization of the GPT-Request to enhance the sentiment analysis for each tweet. To achieve this, I made the following modifications:\n",
        "- Formulated a new prompt to elicit more accurate sentiment responses.\n",
        "- Slightly increased the temperature parameter to introduce more randomness in the generated text and encourage diverse sentiment predictions."
      ],
      "metadata": {
        "id": "oDwqhMdoeZ7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up  OpenAI API credentials\n",
        "openai.api_key = GPT_SECRET_KEY\n",
        "\n",
        "# Set up empty lists to store tweet IDs, tweet texts, and predictions\n",
        "tweet_ids = []\n",
        "tweet_texts = []\n",
        "predictions = []\n",
        "\n",
        "# Iterate over each tweet in the loaded dataset\n",
        "for index, row in english_tweets.iterrows():\n",
        "    tweet_id = row['id']\n",
        "    tweet_text = row['text']\n",
        "    \n",
        "    # Append tweet ID and text to the respective lists\n",
        "    tweet_ids.append(tweet_id)\n",
        "    tweet_texts.append(tweet_text)\n",
        "\n",
        "    # Set up your OpenAI API request\n",
        "    prompt = f\"Given the following tweet, would you say the sentiment expressed is 'positive' or 'negative'? Tweet: '{tweet_text}'\"\n",
        "    \n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-ada-001\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.1\n",
        "    )\n",
        "    \n",
        "    # Extract the predicted sentiment from the OpenAI API response\n",
        "    sentiment = response.choices[0].text.strip()\n",
        "    \n",
        "    # Append the predicted sentiment to the list of predictions\n",
        "    predictions.append(sentiment)\n",
        "    \n",
        "# Create a new DataFrame with tweet IDs, texts, and predictions\n",
        "updated_data = pd.DataFrame({'id': tweet_ids, 'text': tweet_texts, 'query': english_tweets['query'], 'gpt_pred': predictions})\n",
        "\n",
        "# Replace positive/negative in 'gpt_pred' column with 1/0 and '.' with 'N/A'\n",
        "updated_data['gpt_pred'] = updated_data['gpt_pred'].apply(lambda x: 1 if 'positive' in str(x).lower() else (0 if 'negative' in str(x).lower() else 'N/A'))"
      ],
      "metadata": {
        "id": "1aFUtozoWbqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(updated_data)"
      ],
      "metadata": {
        "id": "rM9RUOl4_rtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get a prediction from the LR-Model \n",
        "---\n",
        "\n",
        "I proceed to predict the sentiment of each tweet using my trained logistic regression model. Subsequently, I enhance the dataset by incorporating the sentiment predictions obtained from the model for each respective tweet. This augmentation adds valuable sentiment information to the dataset for further analysis and evaluation."
      ],
      "metadata": {
        "id": "8TJiwybkCLTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the tweets\n",
        "preprocessed_tweets = preprocess(tweet_texts)\n",
        "\n",
        "# Transform the tweets to a numerical representation\n",
        "X = vectorizer.transform(preprocessed_tweets)\n",
        "\n",
        "# Make predictions\n",
        "predictions = LRmodel.predict(X)\n",
        "\n",
        "# Add the predictions to the dataframe\n",
        "updated_data['lr_pred'] = predictions\n"
      ],
      "metadata": {
        "id": "ufeyxX_iCK_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(updated_data)"
      ],
      "metadata": {
        "id": "b8IVJ6krFSNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Data-Frame to a Mongo-DB for later use\n",
        "---\n",
        "\n",
        "All the fetched tweets from the Twitter API, along with the corresponding sentiments obtained from both the GPT model and the linear regression (LR) model, are stored in a MongoDB database. This allows for easy retrieval and utilization of the tweet data and sentiment predictions from both models for subsequent analyses and comparisons."
      ],
      "metadata": {
        "id": "xo7IA_5IFrG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new client and connect to the server\n",
        "client = MongoClient(MONGO_CONNECTION_STRING)\n",
        "\n",
        "db = client.ML2Project\n",
        "\n",
        "# Assuming you want to store the DataFrame in a MongoDB collection named \"mycollection\"\n",
        "collection = db.cryptotweetssentiment\n",
        "\n",
        "# Add a timestamp column to the DataFrame with the current date\n",
        "updated_data['upload_date'] = datetime.now().date().strftime('%Y-%m-%d')\n",
        "\n",
        "# Convert the DataFrame to a list of dictionaries\n",
        "data_dict = updated_data.to_dict(\"records\")\n",
        "\n",
        "# Insert documents into the collection\n",
        "collection.insert_many(data_dict)"
      ],
      "metadata": {
        "id": "vLjnhbeTTGvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load and compare all saved tweets\n",
        "---\n",
        "To create the bar plot, I retrieve the relevant data from the MongoDB collection. I filter the data to include only the instances where the GPT-Prediction and LR-Prediction are identical. Then, I plot the number of positive and negative tweets per day using a bar plot. This allows for a comparison of sentiment distribution between the two models, considering only the instances where their predictions align."
      ],
      "metadata": {
        "id": "us06-34L1ExT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = MongoClient(MONGO_CONNECTION_STRING)\n",
        "db = client.ML2Project\n",
        "collection = db.cryptotweetssentiment\n",
        "queries = collection.distinct(\"query\")\n",
        "\n",
        "bar_width = 0.35\n",
        "\n",
        "for query in queries:\n",
        "    if query in [\"evaluation\", \"new_evaluation\"]:\n",
        "        continue\n",
        "\n",
        "    data = list(collection.find({\"query\": query}))\n",
        "    df = pd.DataFrame(data)\n",
        "    df['upload_date'] = pd.to_datetime(df['upload_date']).dt.date\n",
        "\n",
        "    df_filtered = df[df['lr_pred'] == df['gpt_pred']]\n",
        "\n",
        "    groups = df_filtered.groupby(['upload_date', 'lr_pred'])\n",
        "    dates = []\n",
        "    sentiment_counts = defaultdict(lambda: {'Positive': 0, 'Negative': 0, 'N/A': 0})\n",
        "    for (date, pred), group in groups:\n",
        "        if date not in dates:  # Append date only if it's not already in the list\n",
        "            dates.append(date)\n",
        "        if pred == 1:\n",
        "            sentiment_counts[date]['Positive'] = group.shape[0]\n",
        "        elif pred == 0:\n",
        "            sentiment_counts[date]['Negative'] = group.shape[0]\n",
        "        else:\n",
        "            sentiment_counts[date]['N/A'] = group.shape[0]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    r1 = np.arange(len(dates))\n",
        "    \n",
        "    ax.bar(r1, [sentiment_counts[date]['Positive'] for date in dates], color='green', width=bar_width, label='Positive')\n",
        "    ax.bar(r1, [sentiment_counts[date]['Negative'] for date in dates], color='red', width=bar_width, bottom=[sentiment_counts[date]['Positive'] for date in dates], label='Negative')\n",
        "  \n",
        "\n",
        "    plt.xticks(r1, [str(date) for date in dates], rotation=45)\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_title(f'Sentiment Distribution for {query}')\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), fancybox=True, ncol=3)\n",
        "\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "rlxus5jn1DfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Possible optimizations\n",
        "---\n",
        "To optimize the output and usability of my code, I can consider the following suggestions:\n",
        "\n",
        "1. **I can gather more data from Twitter**: By increasing the amount of data I use for training, I can improve my performance and ability to understand different contexts. Gathering a larger and more diverse dataset from Twitter will enhance my accuracy and enable me to handle a wider range of scenarios.\n",
        "\n",
        "2. **I can explore using a more advanced GPT engine**: If available, I can leverage a better and more sophisticated language model such as GPT-4. Upgrading to a more advanced model can potentially improve my results due to advancements in architecture and training techniques. However, it's important to consider that utilizing such models may come with higher computational costs.\n",
        "\n",
        "3. **I can optimize my tweet retrieval process**: To enhance the quality of the data I use, I can implement filters to exclude bot-generated or irrelevant tweets. By removing noise and focusing on high-quality tweets, I can improve the accuracy and reliability of my predictions.\n",
        "\n",
        "4. **With more computational power, I can train a self-tailored model**: If I have access to additional computational resources, I can train a custom model on a larger dataset. This approach allows me to fine-tune the model specifically to the task at hand, potentially improving my performance and delivering more accurate results.\n",
        "\n",
        "5. **I can provide a user-friendly interface**: By creating a front-end or backend system, I can offer users the ability to select and visualize specific cryptocurrencies of interest. This way, users can focus on the data they find most relevant, improving the usability and making it easier for them to interpret and analyze the results.\n",
        "\n",
        "By implementing these optimizations, I can enhance my overall performance, accuracy, and usability when analyzing and visualizing cryptocurrency data from Twitter."
      ],
      "metadata": {
        "id": "T-eLJTZjkssW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparing the quality of GPT-predictions with a test-data-set\n",
        "---\n",
        " \n",
        "\n",
        "# ***!!!! Please do not execute this code again - the result is saved in a mongo-db !!!!***\n",
        " \n",
        " \n",
        "\n",
        "The gathered data until the 7th of june reveals that the GPT-Request exhibits minimal negative predictions. Consequently, my aim is to evaluate the performance of the GPT Model and the Logistic Regression Model on the labeled test set that is encompassed within the dataset."
      ],
      "metadata": {
        "id": "-9t0KI8l1DMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''#converting the test set to a pandas data frame\n",
        "\n",
        "evaluation_set = dataset['test'].to_pandas()\n",
        "\n",
        "#randomly select 50 tweets\n",
        "\n",
        "def select_random_tweets(dataset, random_seed=42):\n",
        "    # Set the random seed for reproducibility\n",
        "    np.random.seed(random_seed)\n",
        "    \n",
        "    # Separate tweets with sentiment 4 and 0\n",
        "    sentiment_4 = evaluation_set[evaluation_set['sentiment'] == 4]\n",
        "    sentiment_0 = evaluation_set[evaluation_set['sentiment'] == 0]\n",
        "\n",
        "    # Randomly select 25 tweets from each sentiment\n",
        "    random_tweets_4 = sentiment_4.sample(25)\n",
        "    random_tweets_0 = sentiment_0.sample(25)\n",
        "    \n",
        "    # Combine both DataFrames\n",
        "    random_tweets = pd.concat([random_tweets_4, random_tweets_0])\n",
        "    \n",
        "    return random_tweets\n",
        "\n",
        "random_tweets = select_random_tweets(evaluation_data)\n",
        "\n",
        "random_tweets = random_tweets[['text','sentiment']]\n",
        "random_tweets['sentiment'] = random_tweets['sentiment'].replace(4,1)\n",
        "\n",
        "\n",
        "def make_predictions(random_tweets):\n",
        "    # Preprocess the tweets\n",
        "    preprocessed_tweets = preprocess(random_tweets['text'])  # Replace 'tweet_text' with your column name\n",
        "\n",
        "    # Transform the tweets to a numerical representation\n",
        "    X = vectoriser.transform(preprocessed_tweets)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = LRmodel.predict(X)\n",
        "\n",
        "    # Add the predictions to the dataframe\n",
        "    random_tweets['lr_pred'] = predictions\n",
        "\n",
        "    return random_tweets\n",
        "\n",
        "# Now, call the function on your random_tweets\n",
        "updated_random_tweets = make_predictions(random_tweets)\n",
        "\n",
        "# Set up your OpenAI API credentials\n",
        "openai.api_key = GPT_SECRET_KEY\n",
        "\n",
        "# Set up empty lists to store tweet indexes, tweet texts, sentiments, LR predictions, and GPT predictions\n",
        "tweet_indexes = []\n",
        "tweet_texts = []\n",
        "sentiments = []\n",
        "lr_predictions = []\n",
        "gpt_predictions = []\n",
        "\n",
        "# Iterate over each tweet in the loaded dataset\n",
        "for index, row in random_tweets.iterrows():\n",
        "    tweet_index = index\n",
        "    tweet_text = row['text']\n",
        "    sentiment = row['sentiment']\n",
        "    lr_pred = row['lr_pred']\n",
        "\n",
        "    # Append tweet index, text, sentiment, and LR prediction to the respective lists\n",
        "    tweet_indexes.append(tweet_index)\n",
        "    tweet_texts.append(tweet_text)\n",
        "    sentiments.append(sentiment)\n",
        "    lr_predictions.append(lr_pred)\n",
        "\n",
        "    # Set up your OpenAI API request\n",
        "    prompt = f\"Analyze the sentiment of the following tweet: '{tweet_text}'. Answer with 'positive' or 'negative' depending on the sentiment, just one word in the answer\"\n",
        "    \n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-ada-001\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.0\n",
        "    )\n",
        "    \n",
        "    # Extract the predicted sentiment from the OpenAI API response\n",
        "    sentiment = response.choices[0].text.strip()\n",
        "    \n",
        "    # Append the predicted sentiment to the list of GPT predictions\n",
        "    gpt_predictions.append(sentiment)\n",
        "    \n",
        "# Create a new DataFrame with tweet indexes, texts, sentiments, LR predictions, and GPT predictions\n",
        "updated_data = pd.DataFrame({\n",
        "    'index': tweet_indexes,\n",
        "    'text': tweet_texts, \n",
        "    'sentiment': sentiments,\n",
        "    'lr_pred': lr_predictions,\n",
        "    'gpt_pred': gpt_predictions\n",
        "})\n",
        "\n",
        "# Replace positive/negative in 'gpt_pred' column with 1/0 and '.' with 'N/A'\n",
        "updated_data['gpt_pred'] = updated_data['gpt_pred'].apply(lambda x: 1 if 'positive' in str(x).lower() else (0 if 'negative' in str(x).lower() else 'N/A'))\n",
        "\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(MONGO_CONNECTION_STRING)\n",
        "\n",
        "# Connect to the desired database\n",
        "db = client.ML2Project\n",
        "\n",
        "# Connect to the desired collection\n",
        "collection = db.cryptotweetssentiment\n",
        "\n",
        "# Add a new column 'query' with the value 'evaluation' for all rows\n",
        "updated_data['query'] = 'evaluation'\n",
        "\n",
        "# Add a timestamp column to the DataFrame with the current date\n",
        "updated_data['upload_date'] = datetime.now().date().strftime('%Y-%m-%d')\n",
        "\n",
        "# Convert the DataFrame to a list of dictionaries\n",
        "data_dict = updated_data.to_dict(\"records\")\n",
        "\n",
        "# Insert documents into the collection\n",
        "collection.insert_many(data_dict)'''"
      ],
      "metadata": {
        "id": "X5TcXwfHxLGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparing the results of the different models with the sentiment from the testset\n",
        "---\n",
        "\n",
        "Now I compare the sentiments with a confusion matrix for both predictions."
      ],
      "metadata": {
        "id": "Blrk3Vla8va5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new client and connect to the server\n",
        "client = MongoClient(MONGO_CONNECTION_STRING)\n",
        "\n",
        "# Connect to the desired database\n",
        "db = client.ML2Project\n",
        "\n",
        "# Connect to the desired collection\n",
        "collection = db.cryptotweetssentiment\n",
        "\n",
        "# Fetch all documents with query \"evaluation\" and convert them into a DataFrame\n",
        "data = pd.DataFrame(list(collection.find({\"query\": \"evaluation\"})))\n",
        "\n",
        "# Define the target labels and prediction values\n",
        "y_actual = data['sentiment']\n",
        "y_lr_pred = data['lr_pred']\n",
        "y_gpt_pred = data['gpt_pred']\n",
        "\n",
        "# Create confusion matrices\n",
        "cm_lr = confusion_matrix(y_actual, y_lr_pred)\n",
        "cm_gpt = confusion_matrix(y_actual, y_gpt_pred)\n",
        "\n",
        "# Calculate additional metrics\n",
        "lr_sensitivity = cm_lr[1, 1] / (cm_lr[1, 1] + cm_lr[1, 0])\n",
        "lr_specificity = cm_lr[0, 0] / (cm_lr[0, 0] + cm_lr[0, 1])\n",
        "lr_accuracy = (cm_lr[0, 0] + cm_lr[1, 1]) / np.sum(cm_lr)\n",
        "\n",
        "gpt_sensitivity = cm_gpt[1, 1] / (cm_gpt[1, 1] + cm_gpt[1, 0])\n",
        "gpt_specificity = cm_gpt[0, 0] / (cm_gpt[0, 0] + cm_gpt[0, 1])\n",
        "gpt_accuracy = (cm_gpt[0, 0] + cm_gpt[1, 1]) / np.sum(cm_gpt)\n",
        "\n",
        "# Create subplots for the confusion matrices\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Plot LR confusion matrix\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axs[0])\n",
        "axs[0].set_title('Confusion Matrix (LR Predictions)\\n\\nSensitivity (Recall): {:.2%}\\nSpecificity: {:.2%}\\nAccuracy: {:.2%}'.format(lr_sensitivity, lr_specificity, lr_accuracy))\n",
        "axs[0].set_xlabel('Predicted')\n",
        "axs[0].set_ylabel('Actual')\n",
        "\n",
        "# Plot GPT confusion matrix\n",
        "sns.heatmap(cm_gpt, annot=True, fmt='d', cmap='Blues', ax=axs[1])\n",
        "axs[1].set_title('Confusion Matrix (GPT Predictions)\\n\\nSensitivity (Recall): {:.2%}\\nSpecificity: {:.2%}\\nAccuracy: {:.2%}'.format(gpt_sensitivity, gpt_specificity, gpt_accuracy))\n",
        "axs[1].set_xlabel('Predicted')\n",
        "axs[1].set_ylabel('Actual')\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ORXMKxFl8iVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix reveals that the accuracy for the GPT-Prediction is only 50% since every tweet is predicted as positive. This suggests that the prompt needs to be adjusted to improve the GPT model's prediction performance. The modified code, which addresses this issue, is provided above."
      ],
      "metadata": {
        "id": "HWv3l0y2ATUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate tweets again with a optimized GPT-Query\n",
        "---\n",
        "# ***!!!! Please do not execute this code again - the result is saved in a mongo-db !!!!***\n",
        "\n"
      ],
      "metadata": {
        "id": "eE8FyB0gIPB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Setting API Key\n",
        "openai.api_key = GPT_SECRET_KEY\n",
        "\n",
        "# Connecting to MongoDB\n",
        "client = MongoClient(MONGO_CONNECTION_STRING)\n",
        "db = client.ML2Project\n",
        "collection = db.cryptotweetssentiment\n",
        "\n",
        "# Dataframe creation from your dataset\n",
        "evaluation_set = dataset['test'].to_pandas()\n",
        "\n",
        "# Function for selecting random tweets\n",
        "def select_random_tweets(dataset, random_seed=42):\n",
        "    np.random.seed(random_seed)\n",
        "    sentiment_4 = evaluation_set[evaluation_set['sentiment'] == 4]\n",
        "    sentiment_0 = evaluation_set[evaluation_set['sentiment'] == 0]\n",
        "    random_tweets_4 = sentiment_4.sample(25)\n",
        "    random_tweets_0 = sentiment_0.sample(25)\n",
        "    random_tweets = pd.concat([random_tweets_4, random_tweets_0])\n",
        "    return random_tweets\n",
        "\n",
        "random_tweets = select_random_tweets(evaluation_set)\n",
        "random_tweets = random_tweets[['text','sentiment']]\n",
        "random_tweets['sentiment'] = random_tweets['sentiment'].replace(4,1)\n",
        "\n",
        "# Preprocess and predict with LR model\n",
        "preprocessed_tweets = preprocess(random_tweets['text'])\n",
        "X = vectoriser.transform(preprocessed_tweets)\n",
        "predictions = LRmodel.predict(X)\n",
        "random_tweets['lr_pred'] = predictions\n",
        "\n",
        "# Get predictions with GPT\n",
        "tweet_indexes = []\n",
        "tweet_texts = []\n",
        "sentiments = []\n",
        "lr_predictions = []\n",
        "gpt_predictions = []\n",
        "\n",
        "for index, row in random_tweets.iterrows():\n",
        "    tweet_index = index\n",
        "    tweet_text = row['text']\n",
        "    sentiment = row['sentiment']\n",
        "    lr_pred = row['lr_pred']\n",
        "\n",
        "    tweet_indexes.append(tweet_index)\n",
        "    tweet_texts.append(tweet_text)\n",
        "    sentiments.append(sentiment)\n",
        "    lr_predictions.append(lr_pred)\n",
        "\n",
        "    prompt = f\"Given the following tweet, would you say the sentiment expressed is 'positive' or 'negative'? Tweet: '{tweet_text}'\"\n",
        "    \n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-ada-001\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    sentiment = response.choices[0].text.strip()\n",
        "    gpt_predictions.append(sentiment)\n",
        "\n",
        "updated_data = pd.DataFrame({\n",
        "    'index': tweet_indexes,\n",
        "    'text': tweet_texts, \n",
        "    'sentiment': sentiments,\n",
        "    'lr_pred': lr_predictions,\n",
        "    'gpt_pred': gpt_predictions\n",
        "})\n",
        "\n",
        "updated_data['gpt_pred'] = updated_data['gpt_pred'].apply(lambda x: 1 if 'positive' in str(x).lower() else (0 if 'negative' in str(x).lower() else 'N/A'))\n",
        "\n",
        "# Adding evaluation to the 'query' column and today's date to 'upload_date'\n",
        "updated_data['query'] = 'new_evaluation'\n",
        "updated_data['upload_date'] = datetime.now().date().strftime('%Y-%m-%d')\n",
        "\n",
        "# Storing the data in MongoDB\n",
        "data_dict = updated_data.to_dict(\"records\")\n",
        "collection.insert_many(data_dict)'''"
      ],
      "metadata": {
        "id": "zVKM3xM6IaP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Confusion matrix with optimized GPT-Promt\n",
        "---\n",
        "\n",
        "In the new request there are some N/A-Values, those are ignored in the confusion matrix. "
      ],
      "metadata": {
        "id": "YU2acJeRJjKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new client and connect to the server\n",
        "client = MongoClient(MONGO_CONNECTION_STRING)\n",
        "\n",
        "# Connect to the desired database\n",
        "db = client.ML2Project\n",
        "\n",
        "# Connect to the desired collection\n",
        "collection = db.cryptotweetssentiment\n",
        "\n",
        "# Fetch all documents with query \"new_evaluation\" and convert them into a DataFrame\n",
        "data = pd.DataFrame(list(collection.find({\"query\": \"new_evaluation\"})))\n",
        "\n",
        "# Replace 'N/A' values in 'gpt_pred' column with -1\n",
        "data['gpt_pred'] = data['gpt_pred'].apply(lambda x: -1 if x == 'N/A' else x)\n",
        "\n",
        "# Exclude rows with 'N/A' or -1 values in 'gpt_pred' column\n",
        "valid_indices = (data['gpt_pred'] != 'N/A') & (data['gpt_pred'] != -1)\n",
        "data = data[valid_indices]\n",
        "\n",
        "# Define the target labels and prediction values\n",
        "y_actual = data['sentiment']\n",
        "y_lr_pred = data['lr_pred']\n",
        "y_gpt_pred = data['gpt_pred']\n",
        "\n",
        "# Create confusion matrices\n",
        "cm_lr = confusion_matrix(y_actual, y_lr_pred)\n",
        "cm_gpt = confusion_matrix(y_actual, y_gpt_pred)\n",
        "\n",
        "# Calculate additional metrics\n",
        "lr_sensitivity = cm_lr[1, 1] / (cm_lr[1, 1] + cm_lr[1, 0])\n",
        "lr_specificity = cm_lr[0, 0] / (cm_lr[0, 0] + cm_lr[0, 1])\n",
        "lr_accuracy = (cm_lr[0, 0] + cm_lr[1, 1]) / np.sum(cm_lr)\n",
        "\n",
        "gpt_sensitivity = cm_gpt[1, 1] / (cm_gpt[1, 1] + cm_gpt[1, 0])\n",
        "gpt_specificity = cm_gpt[0, 0] / (cm_gpt[0, 0] + cm_gpt[0, 1])\n",
        "gpt_accuracy = (cm_gpt[0, 0] + cm_gpt[1, 1]) / np.sum(cm_gpt)\n",
        "\n",
        "# Create subplots for the confusion matrices\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Plot LR confusion matrix\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axs[0])\n",
        "axs[0].set_title('Confusion Matrix (LR Predictions)\\nAdditional Metrics:\\nSensitivity (Recall): {:.2%}\\nSpecificity: {:.2%}\\nAccuracy: {:.2%}'.format(lr_sensitivity, lr_specificity, lr_accuracy))\n",
        "axs[0].set_xlabel('Predicted')\n",
        "axs[0].set_ylabel('Actual')\n",
        "\n",
        "# Plot GPT confusion matrix\n",
        "sns.heatmap(cm_gpt, annot=True, fmt='d', cmap='Blues', ax=axs[1])\n",
        "axs[1].set_title('Confusion Matrix (GPT Predictions)\\nAdditional Metrics:\\nSensitivity (Recall): {:.2%}\\nSpecificity: {:.2%}\\nAccuracy: {:.2%}'.format(gpt_sensitivity, gpt_specificity, gpt_accuracy))\n",
        "axs[1].set_xlabel('Predicted')\n",
        "axs[1].set_ylabel('Actual')\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5pbF1zrFJi2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix shows, that the accuracy of the GPT-prediction has increased to 91.30% (ignoring the N/A values)"
      ],
      "metadata": {
        "id": "UV1z2uwFQ0DI"
      }
    }
  ]
}