{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLK2yF4mNDqAckdljZVxvy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Welcome to my Python Notebook where I am exploring the fascinating world of cryptocurrency sentiment analysis, focusing particularly on Twitter.\n",
        "\n",
        "Cryptocurrencies, in their volatile and dynamic nature, often exhibit price fluctuations influenced by various factors. One key influencer I am exploring is Twitter, a social media platform where real-time discussions about cryptoassets are abundant.\n",
        "\n",
        "In this project, I will be pulling tweets about different cryptocurrencies using the Twitter API and applying sentiment analysis to these tweets. The objective here is straightforward - to analyze the sentiment embedded in tweets related to specific cryptocurrencies.\n",
        "\n",
        "Join me as we dive into this intriguing mix of Python, cryptocurrencies, Twitter, and sentiment analysis. Let's see what the tweets say!"
      ],
      "metadata": {
        "id": "tAl_DFFnFvXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the credentials\n",
        "\n",
        "A fellow student has to present this project, therefore I implement a function to upload the credentials in a JSON-Format and save them in variables for later use. She or he can request the JSON-File by e-mail from pfeifsas@\n",
        "\n",
        "This process ensures the secure and organized handling of project credentials."
      ],
      "metadata": {
        "id": "a8wLA7fAHMv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import json\n",
        "\n",
        "# Use files.upload to produce the \"Choose Files\" button below, then select your file.\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Use io.BytesIO to decode the file, then json.load to open it.\n",
        "file = io.BytesIO(uploaded['credentials.json'])\n",
        "credentials = json.load(file)\n",
        "\n",
        "# Use Python list comprehension to save each credential to a separate variable.\n",
        "TWITTER_CONSUMER_KEY = credentials['TWITTER_CONSUMER_KEY']\n",
        "TWITTER_CONSUMER_SECRET = credentials['TWITTER_CONSUMER_SECRET']\n",
        "TWITTER_ACCESS_TOKEN = credentials['TWITTER_ACCESS_TOKEN']\n",
        "TWITTER_ACCESS_TOKEN_SECRET = credentials['TWITTER_ACCESS_TOKEN_SECRET']\n",
        "BEARER_TOKEN = credentials['BEARER_TOKEN']\n",
        "GPT_SECRET_KEY = credentials['GPT_SECRET_KEY']\n",
        "MONGO_CONNECTION_STRING = credentials['MONGO_CONNECTION_STRING']\n"
      ],
      "metadata": {
        "id": "Hv5-l7a_IX5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and import all the needed libraries and dependencies\n",
        "\n",
        "With this code all the needed libraries and dependencies are getting installed."
      ],
      "metadata": {
        "id": "EgecDSjgUhth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install die Libraries"
      ],
      "metadata": {
        "id": "cDFMsgKHkZsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install openai\n",
        "!pip install 'pymongo[srv]'"
      ],
      "metadata": {
        "id": "CNdS_gdRkbfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the dependencies"
      ],
      "metadata": {
        "id": "gvMKXXsekUP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# plotting\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# utilities\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.nn import functional as F\n",
        "import openai\n",
        "from pymongo.mongo_client import MongoClient\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n"
      ],
      "metadata": {
        "id": "ZBas_IGja3C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of downloading and importing a CSV I'm using the datasets library to load the initial test-data-set. After loading it, I transform it into a pandas data-set"
      ],
      "metadata": {
        "id": "oPXLEVNYicv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"sentiment140\")\n",
        "\n",
        "# Access a split and convert to a pandas dataframe\n",
        "df = dataset['train'].to_pandas()\n",
        "\n",
        "# Removing the unnecessary columns.\n",
        "df = df[['sentiment','text']]\n",
        "\n",
        "# Replacing the values to ease understanding.\n",
        "df['sentiment'] = df['sentiment'].replace(4,1)\n",
        "\n",
        "# Plotting the distribution for dataset.\n",
        "ax = df.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n",
        "                                               legend=False)\n",
        "ax.set_xticklabels(['Negative','Positive'], rotation=0)\n",
        "\n",
        "# Storing data in lists.\n",
        "text, sentiment = list(df['text']), list(df['sentiment'])"
      ],
      "metadata": {
        "id": "qccGM9saa5aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "MKbCRgYKsDQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining a List of Stop-Words\n",
        "--- "
      ],
      "metadata": {
        "id": "q6xiaqqQd7pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Defining set containing all stopwords in english. \n",
        "\n",
        "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
        "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
      ],
      "metadata": {
        "id": "V9BdstExdBqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining a preprocessing function\n",
        "---\n",
        "The following function is used to preprocess all tweets in preparation for sentiment analysis:"
      ],
      "metadata": {
        "id": "hYwbGydwR_rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(textdata):\n",
        "    processedText = []\n",
        "\n",
        "    # Create Lemmatizer and Stemmer.\n",
        "    wordLemm = WordNetLemmatizer()\n",
        "\n",
        "    # Defining regex patterns.\n",
        "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "    userPattern       = '@[^\\s]+'\n",
        "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
        "    sequencePattern   = r\"(.)\\1\\1+\"\n",
        "    seqReplacePattern = r\"\\1\\1\"\n",
        "    emoticonPattern   = r\"[:;=8][\\-o\\*\\']?[\\)\\]\\(\\[dDpP/\\\\OpP3]\"\n",
        "\n",
        "    for tweet in textdata:\n",
        "        if tweet is not None:   # skip None values\n",
        "            tweet = tweet.lower()\n",
        "\n",
        "            # Replace all URls with 'URL'\n",
        "            tweet = re.sub(urlPattern,' URL',tweet)       \n",
        "            # Replace @USERNAME to 'USER'.\n",
        "            tweet = re.sub(userPattern,' USER', tweet)        \n",
        "            # Replace all non alphabets.\n",
        "            tweet = re.sub(alphaPattern, \" \", tweet)\n",
        "            # Replace 3 or more consecutive letters by 2 letter.\n",
        "            tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
        "            # Replace emoticons with an empty string\n",
        "            tweet = re.sub(emoticonPattern, '', tweet)\n",
        "\n",
        "            tweetwords = ''\n",
        "            for word in tweet.split():\n",
        "                # Checking if the word is a stopword.\n",
        "                if word not in stopwordlist:\n",
        "                    if len(word)>1:\n",
        "                        # Lemmatizing the word.\n",
        "                        word = wordLemm.lemmatize(word)\n",
        "                        tweetwords += (word+' ')\n",
        "                \n",
        "            processedText.append(tweetwords)\n",
        "        \n",
        "    return processedText\n"
      ],
      "metadata": {
        "id": "7o93kjOjeK9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = time.time()\n",
        "processedtext = preprocess(text)\n",
        "print(f'Text Preprocessing complete.')\n",
        "print(f'Time Taken: {round(time.time()-t)} seconds')"
      ],
      "metadata": {
        "id": "qi0C_3yxSkNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating word clouds\n",
        "---\n",
        "\n",
        "### Word-Cloud for negative tweets"
      ],
      "metadata": {
        "id": "jGnJUJMRUmmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_neg = processedtext[:800000]\n",
        "plt.figure(figsize = (20,20))\n",
        "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
        "               collocations=False).generate(\" \".join(data_neg))\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "d0q170rTUpWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word-Cloud for positive tweets"
      ],
      "metadata": {
        "id": "WJxBsaySU1Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_pos = processedtext[800000:]\n",
        "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
        "              collocations=False).generate(\" \".join(data_pos))\n",
        "plt.figure(figsize = (20,20))\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "PUEFPQaWU6EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test-Split"
      ],
      "metadata": {
        "id": "DxANHPZjgZeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n",
        "                                                    test_size = 0.05, random_state = 0)\n",
        "print(f'Data Split done.')\n",
        "\n"
      ],
      "metadata": {
        "id": "Wn6oqImYgYo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF-Vectoriser"
      ],
      "metadata": {
        "id": "kv0RbI7Bgkbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the vectorizer\n",
        "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
        "\n",
        "# Fit it to the training data\n",
        "vectoriser.fit(X_train)\n",
        "\n",
        "print(f'Vectoriser fitted.')\n",
        "print('No. of feature_words: ', len(vectoriser.get_feature_names_out()))"
      ],
      "metadata": {
        "id": "TkLQVID8gjyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming the data set"
      ],
      "metadata": {
        "id": "OxSTL_3po5Nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = vectoriser.transform(X_train)\n",
        "X_test  = vectoriser.transform(X_test)\n",
        "print(f'Data Transformed.')"
      ],
      "metadata": {
        "id": "hZTnxG5lo45g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#function to evaluate different models\n",
        "---\n",
        "This funtion is used to evaluate the trained models"
      ],
      "metadata": {
        "id": "PN4c_WnjjH99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_Evaluate(model):\n",
        "    \n",
        "    # Predict values for Test dataset\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Print the evaluation metrics for the dataset.\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    \n",
        "    # Compute and plot the Confusion matrix\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    categories  = ['Negative','Positive']\n",
        "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
        "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
        "                xticklabels = categories, yticklabels = categories)\n",
        "\n",
        "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
      ],
      "metadata": {
        "id": "iMBWvrmkjLVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Selection Strategy\n",
        "---\n",
        "I am adopting a dual approach for sentiment prediction. The first part of this strategy involves training a custom model, while the second part harnesses the capabilities of the GPT-API for sentiment analysis.\n",
        "\n",
        "Considering the utilization of the GPT-API, I have decided to train a relatively simpler custom model. I'll be training and comparing the performance of three distinct models:\n",
        "\n",
        "- Bernoulli Naive Bayes Model\n",
        "- Linear Support Vector Machine Model\n",
        "- Logistic Regression Model\n",
        "\n",
        "Among these, the model that delivers the highest performance will be selected for further optimization. This optimized model will then be used to predict the sentiments of tweets, which I'll retrieve directly from the Twitter API.\n"
      ],
      "metadata": {
        "id": "wn99XVyiKxLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BernoulliNB Model\n",
        "\n",
        "This is a probabilistic classifier that makes use of Bayes' Theorem with strong independence assumptions. It is particularly suitable for data that can be binary, like the presence or absence of a word in text."
      ],
      "metadata": {
        "id": "JRourgTujVsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BNBmodel = BernoulliNB(alpha = 2)\n",
        "BNBmodel.fit(X_train, y_train)\n",
        "model_Evaluate(BNBmodel)"
      ],
      "metadata": {
        "id": "pHtZpEx0jOnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BernoulliNB-Model has an accuracy of 80%. Let's compare it to a linear support vector machine model."
      ],
      "metadata": {
        "id": "z4odXAiQK9_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear SVM Model\n",
        "\n",
        "This is a maximum-margin classifier which works by constructing a hyperplane or a set of hyperplanes in a high or infinite dimensional space, making it a robust model for text classification tasks."
      ],
      "metadata": {
        "id": "7d1-RxRLpWwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVCmodel = LinearSVC()\n",
        "SVCmodel.fit(X_train, y_train)\n",
        "model_Evaluate(SVCmodel)"
      ],
      "metadata": {
        "id": "vJ1mBJvKpd6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No Improvemnet with the linear support vector machine model. Let's have a look how the performance of a linear regression model looks like."
      ],
      "metadata": {
        "id": "krV5HHCWLbAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Model \n",
        "\n",
        "This is a statistical model that uses a logistic function to model a binary dependent variable. In the context of sentiment analysis, it predicts the probability of a particular sentiment (positive or negative) based on input features."
      ],
      "metadata": {
        "id": "ZcNB4hOqp1_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
        "LRmodel.fit(X_train, y_train)\n",
        "model_Evaluate(LRmodel)"
      ],
      "metadata": {
        "id": "enEjHBzfp1yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The linear regression model has an accuracy of 82%. It's the best performance of all 3 trained models. The performance is ok, but I'll try to improve the accuracy  now."
      ],
      "metadata": {
        "id": "VhW6XCt4MF4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data from the Twitter-API\n",
        "---\n",
        "In this section, I am utilizing the Twitter API to fetch a specific number of tweets. To ensure the quality of the retrieved tweets, I specify a particular currency to be mentioned in the tweets while excluding commonly used spam-bot words. Additionally, I exclude mentions of \"NFT\" and \"NFTs\" as they are not relevant to this project focused on crypto-currencies. This approach helps filter out spam and ensure the retrieved tweets are relevant to the desired topic."
      ],
      "metadata": {
        "id": "JPToCpS5q6up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
        "\n",
        "# Hardcoded query and excluded words to avoid too many spam-tweets from bots\n",
        "query = \"Bitcoin\"\n",
        "excluded_words = ['airdrop', 'bot', 'retweet', 'retweeted', 'wallet', 'mint', 'ticket', 'drop', 'opensea', 'blur', 'NFT', 'NFTs', 'giveaway']\n",
        "\n",
        "# Construct the excluded words portion of the query\n",
        "excluded_query = ' '.join(f'-{word}' for word in excluded_words)\n",
        "\n",
        "# Combine the query and excluded words\n",
        "full_query = f'{query} {excluded_query}'\n",
        "\n",
        "# Set query parameters\n",
        "query_params = {'query': full_query, 'tweet.fields': 'author_id', 'max_results': 10}\n",
        "\n",
        "def bearer_oauth(r):\n",
        "    r.headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n",
        "    r.headers[\"User-Agent\"] = \"v2FilteredStreamPython\"\n",
        "    return r\n",
        "\n",
        "def connect_to_endpoint(url, params):\n",
        "    response = requests.request(\"GET\", url, auth=bearer_oauth, params=params)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(response.status_code, response.text)\n",
        "    return response.json()\n",
        "\n",
        "json_response = connect_to_endpoint(search_url, query_params)\n",
        "\n",
        "# Convert the response to a DataFrame\n",
        "data = [{'id': tweet['id'], 'text': tweet['text'], 'query': query} for tweet in json_response['data']]\n",
        "loaded_tweets = pd.DataFrame(data)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(loaded_tweets)\n"
      ],
      "metadata": {
        "id": "7dXCrm7Z2T6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language recognition\n",
        "---\n",
        "\n",
        "In previous versions of the Twitter API, developers had the capability to select the language of the tweets directly from the API. However, this functionality is no longer available. To overcome this limitation, I am utilizing a pre-trained language detection model to filter and retain only English tweets. This ensures that the tweets used for training and prediction align with the language-specific focus of my model, which is trained exclusively on English tweets."
      ],
      "metadata": {
        "id": "Ze1hEWdz8wpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")"
      ],
      "metadata": {
        "id": "1CYittLB9Zn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function to get a predicted label for the text of a tweets\n",
        "def predict_language(text):\n",
        "    # tokenize the text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    \n",
        "    # run the text through the model and get the logits\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # compute the probabilities from the logits\n",
        "    probabilities = F.softmax(logits, dim=1).detach().numpy()\n",
        "    \n",
        "    # get the label of the highest probability\n",
        "    predicted_label = model.config.id2label[probabilities.argmax()]\n",
        "    \n",
        "    return predicted_label\n",
        "\n",
        "# create a new column with the predicted language\n",
        "loaded_tweets['language'] = loaded_tweets['text'].apply(predict_language)\n",
        "\n",
        "# filter out rows that are not in English\n",
        "english_tweets = loaded_tweets[loaded_tweets['language'] == 'en']\n"
      ],
      "metadata": {
        "id": "NCJ0Us-899Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(english_tweets)"
      ],
      "metadata": {
        "id": "1AJ6HCfhn4aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get a Sentiment from the GPT-API for the english tweets\n",
        "---\n",
        "To obtain the sentiment for each English tweet, I retrieve them individually from the GPT-API. The dataset is then enhanced by adding the sentiment obtained from the API for each respective tweet. The sentiment prediction is encoded as 1 for positive sentiment and 0 for negative sentiment.\n",
        "\n",
        "For the prediction task, I opt to use the \"text-ada-0001\" engine. This decision was primarily driven by financial considerations rather than quality considerations. While the \"Davinci\" model may be more powerful, its cost is approximately 50 times higher than that of the \"text-ada-0001\" model."
      ],
      "metadata": {
        "id": "dNTD9OxC4k_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up your OpenAI API credentials\n",
        "openai.api_key = GPT_SECRET_KEY\n",
        "\n",
        "# Set up empty lists to store tweet IDs, tweet texts, and predictions\n",
        "tweet_ids = []\n",
        "tweet_texts = []\n",
        "predictions = []\n",
        "\n",
        "# Iterate over each tweet in the loaded dataset\n",
        "for index, row in english_tweets.iterrows():\n",
        "    tweet_id = row['id']\n",
        "    tweet_text = row['text']\n",
        "    \n",
        "    # Append tweet ID and text to the respective lists\n",
        "    tweet_ids.append(tweet_id)\n",
        "    tweet_texts.append(tweet_text)\n",
        "\n",
        "    # Set up your OpenAI API request\n",
        "    prompt = f\"Analyze the sentiment of the following tweet: '{tweet_text}'. Answer with 'positive' or 'negative' depending on the sentiment, just one word in the answer\"\n",
        "    \n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-ada-001\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.0\n",
        "    )\n",
        "    \n",
        "    # Extract the predicted sentiment from the OpenAI API response\n",
        "    sentiment = response.choices[0].text.strip()\n",
        "    \n",
        "    # Append the predicted sentiment to the list of predictions\n",
        "    predictions.append(sentiment)\n",
        "    \n",
        "# Create a new DataFrame with tweet IDs, texts, and predictions\n",
        "updated_data = pd.DataFrame({'id': tweet_ids, 'text': tweet_texts, 'query': english_tweets['query'], 'gpt_pred': predictions})\n",
        "\n",
        "# Replace positive/negative in 'gpt_pred' column with 1/0 and '.' with 'N/A'\n",
        "updated_data['gpt_pred'] = updated_data['gpt_pred'].apply(lambda x: 1 if 'positive' in str(x).lower() else (0 if 'negative' in str(x).lower() else 'N/A'))\n"
      ],
      "metadata": {
        "id": "beDHraF35KDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(updated_data)"
      ],
      "metadata": {
        "id": "rM9RUOl4_rtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get a prediction from the LR-Model \n",
        "---\n",
        "\n",
        "I proceed to predict the sentiment of each tweet using my trained linear regression model. Subsequently, I enhance the dataset by incorporating the sentiment predictions obtained from the model for each respective tweet. This augmentation adds valuable sentiment information to the dataset for further analysis and evaluation."
      ],
      "metadata": {
        "id": "8TJiwybkCLTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the tweets\n",
        "preprocessed_tweets = preprocess(tweet_texts)\n",
        "\n",
        "# Transform the tweets to a numerical representation\n",
        "X = vectoriser.transform(preprocessed_tweets)\n",
        "\n",
        "# Make predictions\n",
        "predictions = LRmodel.predict(X)\n",
        "\n",
        "# Add the predictions to the dataframe\n",
        "updated_data['lr_pred'] = predictions\n"
      ],
      "metadata": {
        "id": "ufeyxX_iCK_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(updated_data)"
      ],
      "metadata": {
        "id": "b8IVJ6krFSNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Data-Frame to a Mongo-DB for later use\n",
        "---\n",
        "All the fetched tweets from the Twitter API, along with the corresponding sentiments obtained from both the GPT model and the linear regression (LR) model, are stored in a MongoDB database. This allows for easy retrieval and utilization of the tweet data and sentiment predictions from both models for subsequent analyses and comparisons."
      ],
      "metadata": {
        "id": "xo7IA_5IFrG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new client and connect to the server\n",
        "client = MongoClient(MONGO_CONNECTION_STRING)\n",
        "\n",
        "db = client.ML2Project\n",
        "\n",
        "# Assuming you want to store the DataFrame in a MongoDB collection named \"mycollection\"\n",
        "collection = db.cryptotweetssentiment\n",
        "\n",
        "# Add a timestamp column to the DataFrame with the current date\n",
        "updated_data['upload_date'] = datetime.now().date().strftime('%Y-%m-%d')\n",
        "\n",
        "# Convert the DataFrame to a list of dictionaries\n",
        "data_dict = updated_data.to_dict(\"records\")\n",
        "\n",
        "# Insert documents into the collection\n",
        "collection.insert_many(data_dict)\n"
      ],
      "metadata": {
        "id": "vLjnhbeTTGvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load and compare all saved tweets\n",
        "---\n",
        "Now I'm loading all saved tweets from the mongodb to analyse if the prediction from both models are the same or different. The result is visualised with matplotlib."
      ],
      "metadata": {
        "id": "us06-34L1ExT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = MongoClient(MONGO_CONNECTION_STRING)\n",
        "db = client.ML2Project\n",
        "collection = db.cryptotweetssentiment\n",
        "queries = collection.distinct(\"query\")\n",
        "\n",
        "bar_width = 0.35\n",
        "\n",
        "for query in queries:\n",
        "    data = list(collection.find({\"query\": query}))\n",
        "    df = pd.DataFrame(data)\n",
        "    df['upload_date'] = pd.to_datetime(df['upload_date']).dt.date\n",
        "\n",
        "    groups_gpt = df.groupby(['upload_date', 'gpt_pred'])\n",
        "    dates_gpt = []\n",
        "    sentiment_counts_gpt = defaultdict(lambda: {'Positive': 0, 'Negative': 0, 'N/A': 0})\n",
        "    for (date, pred), group in groups_gpt:\n",
        "        if date not in dates_gpt:  # Append date only if it's not already in the list\n",
        "            dates_gpt.append(date)\n",
        "        if pred == 1:\n",
        "            sentiment_counts_gpt[date]['Positive'] = group.shape[0]\n",
        "        elif pred == 0:\n",
        "            sentiment_counts_gpt[date]['Negative'] = group.shape[0]\n",
        "        else:\n",
        "            sentiment_counts_gpt[date]['N/A'] = group.shape[0]\n",
        "\n",
        "    groups_lr = df.groupby(['upload_date', 'lr_pred'])\n",
        "    sentiment_counts_lr = defaultdict(lambda: {'Positive': 0, 'Negative': 0, 'N/A': 0})\n",
        "    for (date, pred), group in groups_lr:\n",
        "        if pred == 1:\n",
        "            sentiment_counts_lr[date]['Positive'] = group.shape[0]\n",
        "        elif pred == 0:\n",
        "            sentiment_counts_lr[date]['Negative'] = group.shape[0]\n",
        "        else:\n",
        "            sentiment_counts_lr[date]['N/A'] = group.shape[0]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    r1 = np.arange(len(dates_gpt))\n",
        "    \n",
        "    ax.bar(r1, [sentiment_counts_gpt[date]['Positive'] for date in dates_gpt], color='green', width=bar_width, label='Positive (GPT)')\n",
        "    ax.bar(r1, [sentiment_counts_gpt[date]['Negative'] for date in dates_gpt], color='red', width=bar_width, bottom=[sentiment_counts_gpt[date]['Positive'] for date in dates_gpt], label='Negative (GPT)')\n",
        "    ax.bar(r1, [sentiment_counts_gpt[date]['N/A'] for date in dates_gpt], color='gray', width=bar_width, bottom=[sentiment_counts_gpt[date]['Positive'] + sentiment_counts_gpt[date]['Negative'] for date in dates_gpt], label='N/A (GPT)')\n",
        "\n",
        "    ax.bar(r1 + bar_width, [sentiment_counts_lr[date]['Positive'] for date in dates_gpt], color='lightgreen', width=bar_width, label='Positive (LR)')\n",
        "    ax.bar(r1 + bar_width, [sentiment_counts_lr[date]['Negative'] for date in dates_gpt], color='salmon', width=bar_width, bottom=[sentiment_counts_lr[date]['Positive'] for date in dates_gpt], label='Negative (LR)')\n",
        "    ax.bar(r1 + bar_width, [sentiment_counts_lr[date]['N/A'] for date in dates_gpt], color='lightgray', width=bar_width, bottom=[sentiment_counts_lr[date]['Positive'] + sentiment_counts_lr[date]['Negative'] for date in dates_gpt], label='N/A (LR)')\n",
        "\n",
        "    plt.xticks(r1 + bar_width / 2, [str(date) for date in dates_gpt], rotation=45)\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_title(f'Sentiment Distribution for {query}')\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), fancybox=True, ncol=3)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "rlxus5jn1DfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-9t0KI8l1DMN"
      }
    }
  ]
}